{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import dask_geopandas\n",
    "import h3\n",
    "import h3pandas\n",
    "import pandas as pd\n",
    "import requests\n",
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION: download the OBIS snapshot.\n",
    "\n",
    "This checks if the file is available in the directory above. If not, it downloads it.\n",
    "\n",
    "This returns the file path to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download OBIS snapshot\n",
    "def download_obis_snapshot():\n",
    "    url = \"https://api.obis.org/export\"\n",
    "    response = requests.get(url)\n",
    "    json_data = response.json()\n",
    "\n",
    "    # Filter for parquet type\n",
    "    parquets = [item for item in json_data['results'] if item['type'] == 'parquet']\n",
    "    latest_parquet = max(parquets, key=lambda x: x['created'])\n",
    "    s3_path = latest_parquet['s3path']\n",
    "\n",
    "    url_parquet = f'https://obis-datasets.ams3.digitaloceanspaces.com/{s3_path}'\n",
    "    dir_data = os.path.expanduser(\"../\")\n",
    "    file_parquet = os.path.join(dir_data, os.path.basename(url_parquet))\n",
    "\n",
    "    if not os.path.exists(file_parquet):\n",
    "        print(f\"{file_parquet} does not exist! Downloading from {url_parquet}\")\n",
    "        with open(file_parquet, 'wb') as f:\n",
    "            f.write(requests.get(url_parquet).content)\n",
    "    else:\n",
    "        print(f\"{file_parquet} already exists!\")\n",
    "\n",
    "    return file_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION: open the parquet file.\n",
    "\n",
    "Includes optional filters to reduce the amount of data opened for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get OBIS records from snapshot\n",
    "def open_parquet_file(filepath):\n",
    "    print(f'opening {filepath}')\n",
    "    # filepath = \"C:\\\\Users\\\\Mathew.Biddle\\\\Documents\\\\GitProjects\\\\obis_20241201.parquet\"\n",
    "    df = pd.read_parquet(filepath, engine=\"pyarrow\",\n",
    "                         columns=['decimalLongitude', 'decimalLatitude', 'species', 'date_year'],\n",
    "                         #filters=[('species','==','Carcharodon carcharias')], #smaller initial dataset\n",
    "                         )\n",
    "    # dataset = pq.ParquetDataset(filepath)\n",
    "    # table = dataset.read()\n",
    "    # df = table.to_pandas()\n",
    "\n",
    "    # Filter and summarize the data\n",
    "    occ = df.groupby(['decimalLongitude', 'decimalLatitude', 'species', 'date_year']).size().reset_index(name='records')\n",
    "    occ = occ.dropna(subset=['species'])\n",
    "    print(f'{filepath} opened')\n",
    "    return occ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION: load in the US EEZ water boundary shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_us_waters():\n",
    "    print('Load US waters')\n",
    "     # Load US waters shapefile\n",
    "    us_waters_path = \"data/US_Waters_2024_WGS84/US_Waters_2024_WGS84.shp\"\n",
    "    us_waters = gpd.read_file(us_waters_path).set_crs(crs=4326)\n",
    "    us_waters = us_waters.to_crs(epsg=27572)  # Transform to appropriate CRS\n",
    "\n",
    "    return us_waters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION: Subset the occurrence dataframe to US EEZ waters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset2us_waters(occ,us_waters):\n",
    "    print('subset to us waters')\n",
    "    # Subset OBIS records to the region of interest\n",
    "    occ_box = occ[(occ['decimalLatitude'] >= -20) & (occ['decimalLatitude'] <= 80)]\n",
    "    occ_box = occ_box[((occ_box['decimalLongitude'] >= -180) & (occ_box['decimalLongitude'] <= -20)) |\n",
    "                      ((occ_box['decimalLongitude'] >= 120) & (occ_box['decimalLongitude'] <= 180))]\n",
    "\n",
    "    print(\"Convert OBIS occurrences to GeoDataFrame\")\n",
    "    occ_gdf = gpd.GeoDataFrame(occ_box,\n",
    "                               geometry=gpd.points_from_xy(occ_box['decimalLongitude'], occ_box['decimalLatitude']),\n",
    "                               crs=\"EPSG:4326\")\n",
    "    occ_gdf = occ_gdf.to_crs(epsg=27572)\n",
    "\n",
    "    print(\"Check if points are within US waters polygons\")\n",
    "    # occ_in_poly = occ_gdf[occ_gdf.geometry.within(us_waters.unary_union)]\n",
    "\n",
    "    # Perform spatial join to match points and polygons\n",
    "    #occ_in_poly = occ_gdf.overlay(us_waters,how='intersection')\n",
    "    #occ_in_poly = occ_gdf.clip(us_waters)\n",
    "    #occ_in_poly = gpd.tools.sjoin(occ_gdf, us_waters, predicate=\"within\", how='left')\n",
    "\n",
    "    # use DASK\n",
    "\n",
    "    ddf = dask_geopandas.from_geopandas(occ_gdf, npartitions=4)\n",
    "    ddf = ddf.spatial_shuffle()\n",
    "\n",
    "    ddf_us_waters = dask_geopandas.from_geopandas(us_waters, npartitions=4)\n",
    "\n",
    "    ddf_us_waters = ddf_us_waters.spatial_shuffle()\n",
    "    \n",
    "    occ_in_poly = ddf.sjoin(ddf_us_waters,predicate='within',how='inner').compute().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "    return pd.DataFrame(occ_in_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION: Define a function to calculate the various indicators\n",
    "\n",
    "$$shannon = \\sum{-(\\frac{n_i}{n}) * \\ln{(\\frac{n_i}{n})}}$$\n",
    "\n",
    "$$simpson = \\sum{(\\frac{n_i}{n})^2}$$\n",
    "\n",
    "$$maxp = \\max{(\\frac{n_i}{n})}$$\n",
    "\n",
    "$$hill_1 = e^{shannon}$$\n",
    "\n",
    "$$hill_2 = \\frac{1}{simpson}$$\n",
    "\n",
    "$$es50 = \\sum{esi}$$\n",
    "\n",
    "where, `n - n_i >= 50` (with `n` as the total number of records in the cell and `ni` the total number of records for the `i`th-species)\n",
    "\n",
    "$$esi = 1 - e^{\\ln{(n-n_i+1)} + \\ln{(n-50+1)} - \\ln{(n-n_i-50+1)} - \\ln{(n+1)}}$$\n",
    "\n",
    "When, `n >= 50`\n",
    "$$esi = 1$$\n",
    "\n",
    "else,\n",
    "$$esi = NULL$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_indicators(df, esn=50):\n",
    "    print(f'Calculating indicators\\n')\n",
    "    print(df.columns)\n",
    "    # Check that 'df' is a DataFrame and 'esn' is numeric\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"df must be a pandas DataFrame\")\n",
    "    if not isinstance(esn, (int, float)):\n",
    "        raise ValueError(\"esn must be numeric\")\n",
    "\n",
    "    # Check if required columns exist in the DataFrame\n",
    "    required_columns = ['cell', 'species', 'records']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"The DataFrame must contain the columns: {', '.join(required_columns)}\")\n",
    "\n",
    "    # Group by 'cell' and 'species' to calculate ni\n",
    "    df_grouped = df.groupby(['cell', 'species'], as_index=False).agg(ni=('records', 'sum'))\n",
    "\n",
    "    # Calculate total ni for each group\n",
    "    df_grouped['n'] = df_grouped['ni'].sum()\n",
    "\n",
    "    # Calculate hi, si, qi, and esi for each group\n",
    "    df_grouped['hi'] = -(df_grouped['ni'] / df_grouped['n']) * np.log(df_grouped['ni'] / df_grouped['n'])\n",
    "    df_grouped['si'] = (df_grouped['ni'] / df_grouped['n']) ** 2\n",
    "    df_grouped['qi'] = df_grouped['ni'] / df_grouped['n']\n",
    "\n",
    "\n",
    "    # Define the function for esi\n",
    "    def calculate_esi(row):\n",
    "        if row['n'] - row['ni'] >= esn:\n",
    "            return 1 - np.exp(gammaln(row['n'] - row['ni'] + 1) +\n",
    "                              gammaln(row['n'] - esn + 1) -\n",
    "                              gammaln(row['n'] - row['ni'] - esn + 1) -\n",
    "                              gammaln(row['n'] + 1))\n",
    "        elif row['n'] >= esn:\n",
    "            return 1\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "    df_grouped['esi'] = df_grouped.apply(calculate_esi, axis=1)\n",
    "\n",
    "    # Group by 'cell' to summarize the final results\n",
    "    final_results = df_grouped.groupby('cell', as_index=False).agg(\n",
    "        n=('ni', 'sum'),\n",
    "        sp=('species', 'nunique'),\n",
    "        shannon=('hi', 'sum'),\n",
    "        simpson=('si', 'sum'),\n",
    "        maxp=('qi', 'max'),\n",
    "        es=('esi', 'sum')\n",
    "    )\n",
    "\n",
    "    # Calculate hill numbers\n",
    "    final_results['hill_1'] = np.exp(final_results['shannon'])\n",
    "    final_results['hill_2'] = 1 / final_results['simpson']\n",
    "    final_results['hill_inf'] = 1 / final_results['maxp']\n",
    "    print('Done calculating indicators')\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION: Generate H3 gridded data at specified resolution\n",
    "\n",
    "For each cell, calculate the total number of individual species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate h3 indicators at a given resolution\n",
    "def h3_grid(occ, resolution=1):\n",
    "    print('Convert to h3')\n",
    "    # import h3pandas #- https://h3-pandas.readthedocs.io/en/latest/notebook/00-intro.html\n",
    "    # Convert points to h3 cells\n",
    "    # occ['cell'] = occ.apply(lambda row: h3.geo_to_h3(row['decimalLatitude'], row['decimalLongitude'], resolution),\n",
    "    #                         axis=0)\n",
    "\n",
    "    occ.rename(columns={'decimalLongitude':'lng','decimalLatitude':'lat'},inplace=True)\n",
    "    occ = occ.h3.geo_to_h3(resolution).reset_index().rename(columns={f'h3_0{resolution}':'cell'})\n",
    "\n",
    "    # Group by h3 cell and aggregate records\n",
    "    gdf = occ.groupby(['cell','species']).agg({'records': 'sum'}).reset_index(['species']).h3.h3_to_geo_boundary().reset_index()\n",
    "\n",
    "    # grid = occ.groupby('cell').agg({'records': 'sum'}).reset_index()\n",
    "\n",
    "    # # Convert h3 cells to geometries\n",
    "    # grid['geometry'] = grid['cell'].apply(lambda x: h3.h3_to_geo_boundary(x, geo_json=True))\n",
    "\n",
    "    # Convert to GeoDataFrame\n",
    "    #gdf = gpd.GeoDataFrame(grid, geometry='geometry')\n",
    "    gdf.set_crs('EPSG:4326', allow_override=True, inplace=True)\n",
    "\n",
    "    # group by cell index and compute indicators\n",
    "    # idx < - obisindicators::calc_indicators(occ_h3)\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run functions to process data for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../obis_20241202.parquet already exists!\n",
      "opening ../obis_20241202.parquet\n",
      "../obis_20241202.parquet opened\n",
      "Load US waters\n",
      "subset to us waters\n",
      "Convert OBIS occurrences to GeoDataFrame\n",
      "Check if points are within US waters polygons\n",
      "Organizing data processing time: 455.8 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Main function to run the analysis\n",
    "#def run_analysis():\n",
    "# Download OBIS snapshot and load the records\n",
    "file = download_obis_snapshot()\n",
    "occ = open_parquet_file(file)\n",
    "us_waters = load_us_waters()\n",
    "occ_in_poly = subset2us_waters(occ,us_waters)\n",
    "\n",
    "print(f\"Organizing data processing time: {time.time() - start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate occurrences per year for futher analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of occurrences by year\n",
    "occ_year = occ_in_poly.groupby('date_year').size().reset_index(name='occurrence_count')\n",
    "occ_year.to_csv(\"temp/data/occurrence_by_year.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create H3 grid indicators for resolutions 0-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert to h3\n",
      "Calculating indicators\n",
      "\n",
      "Index(['cell', 'species', 'records', 'geometry'], dtype='object')\n",
      "Done calculating indicators\n",
      "Resolution 0 processing time: 24.9 seconds\n",
      "Convert to h3\n",
      "Calculating indicators\n",
      "\n",
      "Index(['cell', 'species', 'records', 'geometry'], dtype='object')\n",
      "Done calculating indicators\n",
      "Resolution 1 processing time: 50.9 seconds\n",
      "Convert to h3\n",
      "Calculating indicators\n",
      "\n",
      "Index(['cell', 'species', 'records', 'geometry'], dtype='object')\n",
      "Done calculating indicators\n",
      "Resolution 2 processing time: 89.3 seconds\n",
      "Convert to h3\n",
      "Calculating indicators\n",
      "\n",
      "Index(['cell', 'species', 'records', 'geometry'], dtype='object')\n",
      "Done calculating indicators\n",
      "Resolution 3 processing time: 169.0 seconds\n",
      "Convert to h3\n",
      "Calculating indicators\n",
      "\n",
      "Index(['cell', 'species', 'records', 'geometry'], dtype='object')\n",
      "Done calculating indicators\n",
      "Resolution 4 processing time: 258.5 seconds\n",
      "Convert to h3\n",
      "Calculating indicators\n",
      "\n",
      "Index(['cell', 'species', 'records', 'geometry'], dtype='object')\n",
      "Done calculating indicators\n",
      "Resolution 5 processing time: 379.7 seconds\n",
      "Convert to h3\n",
      "Calculating indicators\n",
      "\n",
      "Index(['cell', 'species', 'records', 'geometry'], dtype='object')\n",
      "Done calculating indicators\n",
      "Resolution 6 processing time: 556.0 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for resolution in range(0, 7):\n",
    "    \n",
    "    gdf = h3_grid(occ_in_poly, resolution=resolution)\n",
    "    grid_dec = calc_indicators(gdf, esn=50).set_index(\"cell\").h3.h3_to_geo()\n",
    "\n",
    "    grid_dec = gpd.GeoDataFrame(grid_dec.h3.h3_to_geo_boundary().reset_index())\n",
    "    \n",
    "    # Save results as GeoJSON\n",
    "    geojson_string = grid_dec.to_json()\n",
    "\n",
    "    fname = f\"temp/data/indicators_all_res{resolution}.geojson\"\n",
    "    #fname = f\"data/indicators_all_res{resolution}.geojson\"\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write(geojson_string)\n",
    "\n",
    "    print(f\"Resolution {resolution} processing time: {time.time() - start_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTION: Main function to run the analysis\n",
    "\n",
    "commented out for debugging/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_analysis():\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     file = download_obis_snapshot()\n",
    "#     occ = open_parquet_file(file)\n",
    "#     us_waters = load_us_waters()\n",
    "#     occ_in_poly = subset2us_waters(occ,us_waters)\n",
    "\n",
    "#     print(f\"Organizing data processing time: {time.time() - start_time:.1f} seconds\")\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     for resolution in range(0, 7):\n",
    "    \n",
    "#         gdf = h3_grid(occ_in_poly, resolution=resolution)\n",
    "#         grid_dec = calc_indicators(gdf, esn=49).set_index(\"cell\").h3.h3_to_geo()\n",
    "\n",
    "#         grid_dec = gpd.GeoDataFrame(grid_dec.h3.h3_to_geo_boundary().reset_index())\n",
    "    \n",
    "#         # Save results as GeoJSON\n",
    "#         geojson_string = grid_dec.to_json()\n",
    "\n",
    "#         fname = f\"temp/data/indicators_all_res{resolution}.geojson\"\n",
    "#         #fname = f\"data/indicators_all_res{resolution}.geojson\"\n",
    "\n",
    "#         with open(fname, 'w') as f:\n",
    "#             f.write(geojson_string)\n",
    "\n",
    "#         print(f\"Resolution {resolution} processing time: {time.time() - start_time:.1f} seconds\")\n",
    "#     return\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "H3_indicators",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
